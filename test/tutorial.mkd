[[_TOC_]]

# How to run the test suite 


From the exciting root directory, the whole test suite can be run by typing:
```
$ make test
```
In this case the test suite is run with `exciting_smp` as executable. If you want to run the test suite instead with `exciting_serial`, `exciting_purempi` or `exciting_mpismp` you have to use the following command:
```
$ make test_mpi
```
or
```
$ make test_mpiandsmp

```

If you don't want to run the whole test suite, you can explicitly call the script **runtest.py** in directory `test/`.
```
$ cd exciting/test
```
You have to first specify the action you want to execute by using the `-a (--action)` option. In this case you have to type `run` to run parts of the test suite (`clean` to clean the run/ directories). Than you have to specify the executable with the `-e (--executable)` option (e.g. exciting_smp). For an MPI calculation you can set the number of cores used for the MPI run by using `-np (--NP)`. The number of omp threads can be chosen with `-omp (--ompthreads)`. To choose which test cases will be run, you have to use `-t (--tests)` and give the names of test cases divided by spaces. Leaving out `-t` will run all test cases. If you want to propagate assertion failures to the end, you have to add the `-handle-errors` option. That is needed if you want to run all the test cases even if a failure in one of them occurs.
```
$ python3 runtest.py -a run  -e exciting_mpiandsmp -np 3 -t groundstate-GGA_PBE-Ar groundstate-GGA_PBE-properties-Si --handle-errors
```
In this case make sure to clean the `run/` directories after you have run the test suite. Do so by typing:
```
$ python3 runtest.py -a clean -t groundstate-GGA_PBE-Ar groundstate-GGA_PBE-properties-Si
```

# Output generated by the test suite 

If you have done as described in the [section](#how-to-run-the-test-suite) before, you will get an output to stdout. 
At the top you can see the results from the self tests. They check if the test suite itself (e.g. compare functions) works properly. 
```
Run self tests:
test_compare_data_FloatFails (runselftests.TestCompare) ... ok
test_compare_data_FormatFails (runselftests.TestCompare) ... ok
test_compare_data_ListFails (runselftests.TestCompare) ... ok
test_compare_data_NoFails (runselftests.TestCompare) ... ok
test_compare_data_StringFails (runselftests.TestCompare) ... ok
test_path_add (runselftests.TestPath) ... ok
test_path_split (runselftests.TestPath) ... ok

----------------------------------------------------------------------
Ran 7 tests in 0.004s

OK
Self tests SUCCESS
```
Below you can see the results from the actual test suite run. 
```
Run test suite:
Run tests with exciting_smp.
```
First of all you can see whether the run of the test case was successful and how much time the run took. Below that the information on the comparison of the output files are stored. In the first test case all 6 output files are consistent with the references (in the tolerance limits). 
```
Run test groundstate-GGA_PBE-Ar:
Run succeeded
Time (s): 17.8
Report: SUCCESS 6/6, PASS 0/6, FAIL 0/6.
    info.xml SUCCESS
    atoms.xml SUCCESS
    INFO.OUT SUCCESS
    eigval.xml SUCCESS
    evalcore.xml SUCCESS
    geometry.xml SUCCESS
```
In the second test case you can see an assertion failure in INFO.OUT. The type of failure (here: float fail) and path to the location of the failure (here: INFO.OUT/Total energy) are listed too. 
```
Run test groundstate-GGA_PBE-properties-Si:
Run succeeded
Time (s): 109.6
Report: SUCCESS 24/25, PASS 0/25, FAIL 1/25.
    info.xml SUCCESS
    INFO.OUT FAIL
      Failures TOTAL: 1, FORMAT: 0, FLOAT: 1/0, ARRAY: 0/0, STRING 0
        FLOAT FAILURE: Error (1.000e+00) is bigger than tolerance (1.000e-06).
        INFO.OUT/Total energy
    atoms.xml SUCCESS
    eigval.xml SUCCESS
    evalcore.xml SUCCESS
    geometry.xml SUCCESS
    RHO3D.xml SUCCESS
    VCL3D.xml SUCCESS
    VXC3D.xml SUCCESS
    WF3D.xml SUCCESS
    ELF3D.xml SUCCESS
    EF3D.xml SUCCESS
    LSJ.xml SUCCESS
    EFG.xml SUCCESS
    mossbauer.xml SUCCESS
    expiqr.xml SUCCESS
    effmass.xml SUCCESS
    bandstructure.xml SUCCESS
    dos.xml SUCCESS
    KERR.OUT SUCCESS
    EPSILON_11.OUT SUCCESS
    EPSILON_12.OUT SUCCESS
    EPSILON_33.OUT SUCCESS
    CHI_111.OUT SUCCESS
    ELNES.OUT SUCCESS
```
In the end, the test run is summarized. This summary yields the information about the succeeded test calculations and assertions as well as the skipped tests. Skipped tests are tests that do not work properly in the moment. Also the time, the test run took is shown.
```
Summary:
Tests succeeded: 1/2
Assertions succeeded: 30/31
Summary of SKIPPED tests:
  groundstate-LDA_PW-gw-Si .  MPI GW calculations do not produce EPS00_GW.OUT
Total test suite time (mins) : 2.1
Average test time (s): 63.7
Longest test time (s): 109.6, taken by groundstate-GGA_PBE_properties-Si

```

# How to add a new test case

## Quick Guide on generating a new test case

To add a new test case, enter the directory `EXCITINGROOT/test/` then run:
```
$ python3 newtestcase.py -n name -d description -r path/to/calculation
```
to copy reference input files from `path/to/calculation` to `EXCITINGROOT/test/test_farm/name/ref`.
You can find the suggested naming convention [here](#generating-a-new-test-case).

**Note**: You can also leave out all options. You will then be asked to give each argument one by one. 

Tolerances for regression tests also need to be defined in `EXCITINGROOT/test/test_farm/name/init_<method>.xml`. 
Several templates can be found in `EXCITINGROOT/test/init_templates`, and require the developer to:
- Copy the required init files
- Set the tolerances as appropriate

You can find more information on this in the section [Closer look at init files](#closer-look-at-init-files).

All tests in `EXCITINGROOT/test/test_farm` are automatically included in the test suite.


## Generating a new test case

After you have entered the `test` directory you can use the python script **newtestcase.py** to add a new test case.

```
$ python3 newtestcase.py -n name -d description -r path/to/calculation
```

You can specify the name of the test case by using the `-n (--name)` option. Therefore please have a look at the [naming convention](#naming-convention).
A more detailed description on the calculation and the test has to be given in the description option `-d (--description)`.
If you have already stored your test case calculation in some directory, you can simply use the `-r (--reference)` option and give the path to your calculation to add the species and input files automatically. A reference calculation for the test case is then started immediately.
If you prefer to add species and input files by hand, you can just skip the `-r` option.
In this case you have to use the **runtest.py** script to run the reference calculation. 

```
$ python3 runtest.py -a ref -t name_of_your_test_case
```

If you have done so, a new directory in the test_farm directory is produced. There you will find an init file and two directories called `ref/` and `run/`. 
The `ref/` directory contains input files and results of the reference calculation. The `run/` directory contains only species and input files. This is the place where the test calculation is run.
The init file defines which output files will be compared to a reference and the corresponding tolerances. You'll get more information about init files [here](#closer-look-at-init-files).

Now a basic version of your test case is ready to use. Only groundstate output files are compared to the reference and pre-set tolerances are used by default.

## Naming convention

The following naming convention is required:
**groundstate-xctype-postprocessing/MBPT-material**.
Where **postprocessing/MBPT** stands for everything that comes on top of the ground state calculation, e.g. bandstructure or BSE. Each section is divided by a "-" and if more than one word is used to describe a section, they are divided by a "_" (e.g. GGA_PBE for xctype). In general the name should give a brief overview on what is calculated. For example you want to use a calculation of LiF with GGA_PBE as exchange-correlation functional and BSE on top of the groundstate calculation as a new test case. Then you have to name your test case: *groundstate-GGA_PBE-tddft-LiF*.

## Closer look at init files

In addition to the default ground state init file, there are several different pre-built init files that cover more output files, than just the groundstate ones:

- init_bandstructure.xml
- init_BSE.xml
- init_dos.xml
- init_GW.xml
- init_properties.xml
- init_spintexture.xml
- init_wannier.xml

A list of output files defined in each init file can be found in the README.
These more advanced init files can be added in the beginning by using `-i (--init_file)` while running the **newtestcase.py**.

```
$ python3 newtestcase.py -n name -d description -i init_file.xml
```

If you want to add or remove single output files from the init file or adjust the tolerances, you have to do this manually. Therefore you need to understand the basic structure of an init file.

At the top of the init file the name and description you chose for the test case is defined. Followed by the executable for the reference calculation (is always exciting_smp). 
Below you find several `test` elements. In each of those elements one output file is defined. Therefore five attributes are used:

- `file`: name of the output file.
- `tolFloat`: absolute tolerance, used for float values.
- `tolMSE`: absolute tolerance, used for the mean squared errors of value lists.
- `condition`: (float fails, mse fails) defines how many float and mse fails are allowed. Default is zero for both. It should be explained in the description if not so. 
- `ignore`: list of variables that shouldn't be compared to a reference, divided by semicolons. This could be numerically varying 
or system dependent values (e.g. timing). 

```xml
<init name        = 'test_name'
      description = 'test_description'
      executable  = 'exciting_smp'>
  <test   file      = 'atoms.xml'
          tolFloat  = '1e-6'
          tolMSE    = '1e-6'
          condition = '0 0'
          ignore    = 'timing;rmslog10'/>
  <test   file      = 'evalcore.xml'
          tolFloat  = '1e-6'
          tolMSE    = '1e-6'
          condition = '0 0'
          ignore    = ''/>
</init>

```

In addition to these five attributes there is an optional child element called `tolValuePair`, it allows you to define a specific tolerance for one value:

```xml
<tolValuePair
     value = 'Dos at Fermi energy (states/Ha/cell)'
     tol   = '1e-5'/>
```

For some output files other specific attributes have to be added.
- **info.xml** and **INFO.OUT**:
For those two output files you have to add the required attribute `tolIter`, in which a tolerance for the number of self consistent loop iterations is defined. You have as well the possibility to add another attribute called `maxIter` that specifies a maximal number of allowed self consistent loop iterations. When this attribute is used, the `tolIter` attribute will be ignored. 

```xml
<test file      = 'info.xml'
          tolFloat  = '1e-6'
          tolMSE    = '1e-6'
          condition = '0 0'
          ignore    = 'timing;rmslog10'
          tolIter   = '3'
	  maxIter   = '25'/>
 <test   file      = 'INFO.OUT'
          tolFloat  = '1e-6'
          tolMSE    = '1e-6'
          condition = '0 0'
          ignore    = 'Wall time (seconds);CPU time for vxnl (seconds);CPU time for scf_cycle (seconds)'
          tolIter   = '3'
	  tolIter   = '25'/>
```

- **eigval.xml**:
By using the `eigval` child element, you can compare differences of eigenvalues to a reference. In the `eigvalPair` attribute the pairs of eigenvalues that are subtracted from each other are chosen (e.g. degenerate eigenvalues). These can be extracted from EIGVAL.OUT. In the `tolEigval` attribute the corresponding tolerance is defined. 

```xml
 <test   file      = 'eigval.xml'
          tolFloat  = '1e-3'
          tolMSE    = '1e-3'
          condition = '0 0'
          ignore    = ''>
          <eigval
              eigvalPair = '1,2 6,7 4,5 5,8 9,10'
              tolEigval   = '1e-8'/>
       </test>
```

If you want to add or remove an output file from the init file, you have to add or delete the whole `test` element with its attributes and child elements.
If you add a new output file, you have to make sure that a parser for this output file already exists. Therefore you have to enter the following directory:

```
$ cd exciting/test/tools/parser
```
Here you will find **parserChooser.py** in which all parsed files and the corresponding parsers are mentioned. If you can't find a parser for your output file, you have to write a new one. The parser should be written in python. As input it should take the path to the output file and return the data in the form of a python dictionary. If you have done so, please ask Benedikt Maurer or Hannah Kleine to add the parser to the test suite.

**Note:** This is only an interim solution. In the future the parsers from NOMAD will be used for the test suite. Please contact Alvin Noe Ladines with regards to making additions to the NOMAD exciting parser.

# Requirements

The test suite requires python3 and the following packages:

- argparse
- collections
- numpy
- xml
- unittest

In addition you can use *termcolor* for colorful terminal output.

For receiving python3 on a Unix system prompt:
```
$ sudo apt-get update
$ sudo apt-get install python3.x
```
`x` depends on the exact version of python3 you want to use.
All standard packages should be shipped within python3 already. 
To install needed python packages, the easiest way is to use python3 package installer pip3:
```
$ pip3 update
$ pip3 install <package>
```
Of course you can use any source package management system like conda as well.
 

# Skipped tests

There are only a few cases in which it is appropriate to add a test case to list of failing tests:


1. If someone writes a test for existing code that fails, because this highlights a problem that we were previously unaware of, and the failing test demonstrates what inputs cause it to fail. The person may however, not be an expert in that area of the code, have other priorities, or the fix is complex. Hence why it is not fixed immediately.

2. When someone is developing a new feature that's not aimed at stable release. They may have it to a point where a subset of tests run, but another subset fails and they're actively working on the problem but want to regularly merge into the main development branch.

3. Failing tests for new features should not be merged into a stable release, unless it's completely unavoidable (someone else needs stable parts of a larger feature). Even then, one may argue that the merge should just be split into smaller parts.

In all of these cases it is crucial to add an issue to GitLab, in order to keep track of the problem. 

For the test run to skip a test defined in the test_farm, you have to add it to the list of failing tests. Therefore you have to enter the `test/` directory and open **failing_tests.py**. You have to add an entry to the `failing_tests` list. This entry should be a python dictionary and have following format:

```python3
# TODO(name of responsible developer) Issue: number of the GitLab issue
    {'name': 'name_of_the_test_case',
     'comment':'some comment',
     'tags': [CompilerBuild(Compiler.nameofcompiler, Build_type.buildtype)]
    },

```


